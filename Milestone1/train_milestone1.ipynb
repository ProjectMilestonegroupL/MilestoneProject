{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Imports",
   "metadata": {
    "tags": [],
    "cell_id": "00001-1da2ae5a-c906-423b-a069-499ab4ba6fc6",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-f0416702-bac4-4b9b-9ef7-fcd8b89ba78b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "33f4f6e",
    "execution_start": 1622218489757,
    "execution_millis": 7563,
    "deepnote_cell_type": "code"
   },
   "source": "from typing import Tuple, List\n\n# import librairies\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# import machine learning packages\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import MNIST, FashionMNIST\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch\nimport torchvision\n\n\n",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Data recuperation",
   "metadata": {
    "tags": [],
    "cell_id": "00003-239c8af4-6807-493e-a77e-5ddadb557c80",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00002-dfbe287d-b846-4b1e-92d5-222aaf1f9d62",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "66eae2d2",
    "execution_start": 1622218497329,
    "execution_millis": 546,
    "deepnote_cell_type": "code"
   },
   "source": "# recuperate the complete data set\nX_train_all = pd.read_csv('train_set.csv')\nX_val_all=pd.read_csv('val_set.csv')\nX_test_all=pd.read_csv('test_set.csv')\n\n# convert into numpy arrays\nX_train_all= X_train_all.to_numpy()\nX_val_all= X_val_all.to_numpy()\nX_test_all=X_test_all.to_numpy()\n\n\nprint('Training set shape:')\nprint(f'X: {X_train_all.shape} ')\n\nprint('\\nVal set shape:')\nprint(f'X: {X_val_all.shape} ')\n\nprint('\\nTest set shape:')\nprint(f'X: {X_test_all.shape} ')\n",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Training set shape:\nX: (12646, 111) \n\nVal set shape:\nX: (1500, 111) \n\nTest set shape:\nX: (3000, 110) \n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Data reshape and transformation",
   "metadata": {
    "tags": [],
    "cell_id": "00009-2538e82b-4145-4972-8e72-919542f6fe35",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00008-eec58721-58c0-43f1-8dba-1170396b7aad",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cb09ca1a",
    "execution_start": 1622218497885,
    "execution_millis": 21,
    "deepnote_cell_type": "code"
   },
   "source": "# splitting our data for the training in input (X_train) and output (y_train)\nX_train= X_train_all[:,0:110].astype(np.float32)\ny_train= X_train_all[:,110].astype(np.float32)\n\n\n# splitting our data for the validation in input (X_val) and output (y_val)\nX_val= X_val_all[:,0:110].astype(np.float32)\ny_val= X_val_all[:,110].astype(np.float32)\n'''\nThese data are equivalent to test_set in the conventions but we use them\nto get an idea of the performance before having the test_set evaluated on AIcrowd \n'''\n\n# separation of our data for the test set into input (X_test)\nX_test= X_test_all.astype(np.float32)\n\n\n# transform numpy array in a TensorDataset\n'''\nUse unsqueeze to counter this error msg  : UserWarning: Using a target size (torch.Size([1])) that is different \nto the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting.\n'''\ntrain = torch.utils.data.TensorDataset(torch.from_numpy(X_train),torch.unsqueeze(torch.from_numpy(y_train),1))\nvalidation = torch.utils.data.TensorDataset(torch.from_numpy(X_val),torch.unsqueeze(torch.from_numpy(y_val),1))\n\ntest=torch.from_numpy(X_test)\n\n# separate features and targets of the TensorDataset \ntrain_loader = torch.utils.data.DataLoader(train, batch_size=12, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(validation, batch_size=1, shuffle=True)\n\n",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Neural network",
   "metadata": {
    "tags": [],
    "cell_id": "00008-763caefb-a1df-4991-9969-257e399748ea",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "To prevent overfitting, we use a dropout in the model.",
   "metadata": {
    "tags": [],
    "cell_id": "00008-5d7956c7-2aca-482d-9389-e22fd60efebd",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-2fc9b8f3-feeb-4d55-9efc-9bb09b46c739",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "52e5eec9",
    "execution_start": 1622218497911,
    "execution_millis": 18,
    "deepnote_cell_type": "code"
   },
   "source": "# define our model\n\nclass model(nn.Module):\n    \n    # fully connected neural network\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(110,66) \n        self.fc2 = nn.Linear(66,20)\n        self.dropout = nn.Dropout(0.1)\n        self.fc3 = nn.Linear(20,1)\n        \n    # architecture of our model and activation function\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = F.softsign(self.fc1(x)) \n        x = F.softsign(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n  \n        return x\n\nmodel=model()",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Loss &amp; optimizer",
   "metadata": {
    "tags": [],
    "cell_id": "00010-3ea08dff-09e6-4683-b722-98454effd619",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00008-1816fb52-e2fd-4386-b1d5-4a5258d03672",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d516eda",
    "execution_start": 1622218497935,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "loss_fn =nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n\noptimizer = optim.RMSprop(model.parameters(),lr=0.0011)\n#optimizer = optim.Adadelta(model.parameters(), lr=0.005, rho=0.95, eps=1e-06, weight_decay=0)\n#optimizer = optim.AdamW(model.parameters(),lr=0.003,betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False)\n#optimizer = optim.Adagrad(model.parameters(), lr=0.004, lr_decay=0.00001, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n#optimizer = optim.Adam(model.parameters(),lr=0.003,betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False)\n",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Loss metric function ",
   "metadata": {
    "tags": [],
    "cell_id": "00012-1800966f-8b2b-4ae7-b7cd-1c9c010e59f9",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "We import this function from a course exercice : https://github.com/vita-epfl/introML-2021/blob/main/exercises/06-neural-nets/metrics.py",
   "metadata": {
    "tags": [],
    "cell_id": "00012-d052f541-17ba-46cf-a27c-1e08311b2990",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-3518b0bd-ff11-4590-9654-98a321030064",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fb6621b",
    "execution_start": 1622218497944,
    "execution_millis": 42,
    "deepnote_cell_type": "code"
   },
   "source": "# compute the loss and keeps track of the loss over an epoch\nclass LossMetric:\n\n    def __init__(self) -> None:\n        self.running_loss = 0\n        self.count = 0\n\n    def update(self, loss: float, batch_size: int) -> None:\n        self.running_loss += loss * batch_size\n        self.count += batch_size\n\n    def compute(self) -> float:\n        return self.running_loss / self.count\n\n    def reset(self) -> None:\n        self.running_loss = 0\n        self.count = 0",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Model training",
   "metadata": {
    "tags": [],
    "cell_id": "00014-4ef3022d-603c-4075-a889-3f1ca38ee669",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00008-7477d3f9-b7b4-4c52-8c0f-663db1fc2c76",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6141bda0",
    "execution_start": 1622218497987,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "def train(model: torch.nn.Module, train_loader : torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, epochs: int):\n    \n    # initialization of the loss\n    loss_metric = LossMetric()\n    \n    # sets the module in training mode\n    model.train()\n    \n    for epoch in range(1,epochs+1): \n\n        # iterate through data\n        for data, target in train_loader:\n            \n            # zero-out the gradients\n            optimizer.zero_grad()\n\n            # forward pass\n            out = model(data)\n            \n            # compute the loss\n            loss = loss_fn(out,target)\n            \n            # backward pass\n            loss.backward()\n            \n            # optimizer step\n            optimizer.step()            \n            \n            # update metrics\n            loss_metric.update(loss.item(), data.shape[0])\n        \n       \n        # end of epoch, show loss\n        print(\"Train loss :\"+ str(loss_metric.compute()))\n        \n        loss_metric.reset()",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-8a8a1d47-e48e-418b-af3f-5d68e306af48",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e2cc4d15",
    "execution_start": 1622218497987,
    "execution_millis": 69406,
    "deepnote_cell_type": "code"
   },
   "source": "train(model, train_loader, loss_fn, optimizer, epochs=50)",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train loss :0.241384721259617\nTrain loss :0.19489124605484293\nTrain loss :0.1864510549156656\nTrain loss :0.18483087034030238\nTrain loss :0.1795970274878702\nTrain loss :0.17786516900207167\nTrain loss :0.17602475121718653\nTrain loss :0.17366573872156918\nTrain loss :0.17273469367077054\nTrain loss :0.17077453625828107\nTrain loss :0.16885798047711328\nTrain loss :0.16878369662815845\nTrain loss :0.16732871333184582\nTrain loss :0.1662424870828709\nTrain loss :0.16480386687203163\nTrain loss :0.16553187980174444\nTrain loss :0.16302973325666975\nTrain loss :0.16382104033940384\nTrain loss :0.16116583564310027\nTrain loss :0.16106468824592832\nTrain loss :0.15868962058043642\nTrain loss :0.1598027742248504\nTrain loss :0.15897073837861161\nTrain loss :0.15845278814192268\nTrain loss :0.15830271278403282\nTrain loss :0.15744320984570997\nTrain loss :0.15810405183303644\nTrain loss :0.15650036613743093\nTrain loss :0.15534775525761\nTrain loss :0.15436825447053157\nTrain loss :0.15461603258286458\nTrain loss :0.1551364176122614\nTrain loss :0.1542331003301905\nTrain loss :0.15339444035691596\nTrain loss :0.15306059019081855\nTrain loss :0.15370020489516975\nTrain loss :0.15290598060995242\nTrain loss :0.15264353461129962\nTrain loss :0.15237992384640237\nTrain loss :0.15181216625689697\nTrain loss :0.1513650210360108\nTrain loss :0.15153188638803883\nTrain loss :0.1515322551525989\nTrain loss :0.15282863874719543\nTrain loss :0.15078814221755502\nTrain loss :0.15118345018036142\nTrain loss :0.15070581411265005\nTrain loss :0.15094255865393194\nTrain loss :0.15058544137923566\nTrain loss :0.15056577835143056\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Model evaluation",
   "metadata": {
    "tags": [],
    "cell_id": "00018-76ad237e-c6df-4235-898f-3c04d6c997b4",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00011-6dc96436-71ea-41e3-ba04-7261394efafb",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5ce26495",
    "execution_start": 1622218567402,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "def validation(model: torch.nn.Module, validation_loader : torch.utils.data.DataLoader):\n     \n    MSEloss= nn.MSELoss(reduction='none') \n    model.eval()\n    \n    loss_metric= []\n\n    with torch.no_grad(): \n\n        # iterate through data\n        for data, target in validation_loader:\n            \n            # forward pass and update our loss_metrics array with .tolist()\n            out = model(data)\n            loss_metric += MSEloss(out, target).tolist()\n            \n    # transform list in array to use \"sum\"\n    loss_metric_array = np.asarray(loss_metric) \n    \n    # mean of the loss\n    final_loss = (sum(loss_metric_array)/len(loss_metric))\n    print((final_loss))\n    ",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00012-4b6ee795-6a80-4ab1-aa9f-3f84923c122d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "22d5650c",
    "execution_start": 1622218567407,
    "execution_millis": 487,
    "deepnote_cell_type": "code"
   },
   "source": "validation(model, validation_loader) \n",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "[0.14333816]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#  .csv submission file generation",
   "metadata": {
    "tags": [],
    "cell_id": "00021-c305b8a4-6b86-43d5-ba08-9ea8713f05d6",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00014-312f0091-40d9-4a84-82be-8de76290e1c2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "57b47345",
    "execution_start": 1622218567897,
    "execution_millis": 56,
    "deepnote_cell_type": "code"
   },
   "source": "ok = model(test).detach().numpy()\n\ndf = pd.DataFrame(ok,columns=[\"sat1_col\"])\nprint(df)\nprint('***')\ndf.to_csv(\"sat1_col.csv\", index = False)\n",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "      sat1_col\n0     0.747528\n1     1.285604\n2     1.298330\n3     1.413915\n4     2.439812\n...        ...\n2995  0.747444\n2996  1.139007\n2997  1.251950\n2998  1.160040\n2999  1.999687\n\n[3000 rows x 1 columns]\n***\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7069e5d5-d80e-47ef-99c8-07f7aae62cb7' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "fd49d502-7ea2-4b04-a03f-a7585778d4ba",
  "deepnote_execution_queue": []
 }
}